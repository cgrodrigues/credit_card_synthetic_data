{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "transactions_df = pd.read_csv(\"./data/transactions_enrich_df.csv\", parse_dates = ['post_ts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn - RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9723759463122748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/scikit-learn-random-forest-model/scikit-learn-random-forest-model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##------------------------------------------\n",
    "##             Scikit-Learn   \n",
    "##------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "data = transactions_df[transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")]\n",
    "\n",
    "# Separate features (F1, F2, F3) and target (TARGET)\n",
    "columns =  list(transactions_df.columns)\n",
    "\n",
    "# Entries you want to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode', \n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                        'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                        'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                        ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "target = 'fraud'\n",
    "y = data[target]\n",
    "\n",
    "# Define the traing range max limit\n",
    "end_training = pd.Timestamp('2023-05-31')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = X[X['post_ts'] <= end_training]\n",
    "y_train = y.loc[X_train.index]\n",
    "X_test = X[X['post_ts'] > end_training]\n",
    "y_test =  y.loc[X_test.index]\n",
    "\n",
    "# Drop teh column post_ts\n",
    "X_train = X_train.drop(columns=['post_ts'])\n",
    "X_test = X_test.drop(columns=['post_ts'])\n",
    "\n",
    "# Save feature names and target name to a JSON file\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'target': target,\n",
    "    'model_name': 'RandomForestClassifier Model Example',\n",
    "    'model_version': 1.0,\n",
    "    'model_type':  'Scikit-Learn'\n",
    "}\n",
    "\n",
    "with open('./models/scikit-learn-random-forest-model/scikit-learn-random-forest-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, './models/scikit-learn-random-forest-model/scikit-learn-random-forest-model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud:0 - No Fraud:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = joblib.load('./models/scikit-learn-random-forest-model/scikit-learn-random-forest-model.pkl')\n",
    "# Read the metadata file\n",
    "with open('./models/scikit-learn-random-forest-model/scikit-learn-random-forest-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    {'amt':[141.45], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[3], \n",
    "            'customer_id_avg_amount_1day_window':[88.08],\n",
    "            'customer_id_nb_tx_7day_window':[20.0], \n",
    "            'customer_id_avg_amount_7day_window':[64.4855],\n",
    "            'customer_id_nb_tx_30day_window':[82.0], \n",
    "            'customer_id_avg_amount_30day_window':[59.64829268292683],\n",
    "            })\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "# Get the cluster assignment for the new record\n",
    "prediction_fraud = model.predict(new_record_fraud)[0]\n",
    "prediction_no_fraud = model.predict(new_record_no_fraud)[0]\n",
    "\n",
    "\n",
    "print (f\"Fraud:{prediction_fraud} - No Fraud:{prediction_no_fraud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amt</td>\n",
       "      <td>0.247310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>customer_id_avg_amount_30day_window</td>\n",
       "      <td>0.203065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>customer_id_avg_amount_7day_window</td>\n",
       "      <td>0.166842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>customer_id_avg_amount_1day_window</td>\n",
       "      <td>0.154899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer_id_nb_tx_30day_window</td>\n",
       "      <td>0.094059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>customer_id_nb_tx_7day_window</td>\n",
       "      <td>0.063439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>customer_id_nb_tx_1day_window</td>\n",
       "      <td>0.050415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>during_weekend</td>\n",
       "      <td>0.014414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>during_night</td>\n",
       "      <td>0.005558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               feature  importance\n",
       "0                                  amt    0.247310\n",
       "8  customer_id_avg_amount_30day_window    0.203065\n",
       "6   customer_id_avg_amount_7day_window    0.166842\n",
       "4   customer_id_avg_amount_1day_window    0.154899\n",
       "7       customer_id_nb_tx_30day_window    0.094059\n",
       "5        customer_id_nb_tx_7day_window    0.063439\n",
       "3        customer_id_nb_tx_1day_window    0.050415\n",
       "1                       during_weekend    0.014414\n",
       "2                         during_night    0.005558"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature importances\n",
    "model = joblib.load('./models/scikit-learn-random-forest-model/scikit-learn-random-forest-model.pkl')\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Separate features (F1, F2, F3) and target (TARGET)\n",
    "columns =  list(transactions_df.columns)\n",
    "\n",
    "# Entries you want to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                        'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                        'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window',\n",
    "                        'post_ts'\n",
    "                        ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "# Convert the importances into a DataFrame\n",
    "feature_importance_df = pd.DataFrame({'feature': features, 'importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "feature_importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9719147740421171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/xgboost-model/xgboost-model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = transactions_df[transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")]\n",
    "\n",
    "# Separate features and target\n",
    "columns = list(transactions_df.columns)\n",
    "\n",
    "# Entries to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                     'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                     'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                    ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "target = 'fraud'\n",
    "y = data[target]\n",
    "\n",
    "# Define the training range max limit\n",
    "end_training = pd.Timestamp('2023-05-31')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = X[X['post_ts'] <= end_training]\n",
    "y_train = y.loc[X_train.index]\n",
    "X_test = X[X['post_ts'] > end_training]\n",
    "y_test = y.loc[X_test.index]\n",
    "\n",
    "# Drop the 'post_ts' column\n",
    "X_train = X_train.drop(columns=['post_ts'])\n",
    "X_test = X_test.drop(columns=['post_ts'])\n",
    "\n",
    "# Save feature names and target name to a JSON file\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'target': target,\n",
    "    'model_name': 'XGBoost Model Example',\n",
    "    'model_version': 1.0,\n",
    "    'model_type': 'XGBoost'\n",
    "}\n",
    "\n",
    "with open('./models/xgboost-model/xgboost-model-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "clf = XGBClassifier(n_estimators=10, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, './models/xgboost-model/xgboost-model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud:0 - No Fraud:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = joblib.load('./models/xgboost-model/xgboost-model.pkl')\n",
    "# Read the metadata file\n",
    "with open('./models/xgboost-model/xgboost-model-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    {'amt':[141.45], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[3], \n",
    "            'customer_id_avg_amount_1day_window':[88.08],\n",
    "            'customer_id_nb_tx_7day_window':[20.0], \n",
    "            'customer_id_avg_amount_7day_window':[64.4855],\n",
    "            'customer_id_nb_tx_30day_window':[82.0], \n",
    "            'customer_id_avg_amount_30day_window':[59.64829268292683],\n",
    "            # 'terminal_id_nb_tx_1day_window':[43.0], \n",
    "            # 'terminal_id_risk_1day_window':[0.09302325581395349],\n",
    "            # 'terminal_id_nb_tx_7day_window':[233.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.04721030042918455],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1013.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.03060217176702863]\n",
    "            })\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            # 'terminal_id_nb_tx_1day_window':[31.0], \n",
    "            # 'terminal_id_risk_1day_window':[0],\n",
    "            # 'terminal_id_nb_tx_7day_window':[252.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.03571428571428571],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1166.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.0274442538593482]\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "# Get the cluster assignment for the new record\n",
    "prediction_fraud = model.predict(new_record_fraud)[0]\n",
    "prediction_no_fraud = model.predict(new_record_no_fraud)[0]\n",
    "\n",
    "\n",
    "print (f\"Fraud:{prediction_fraud} - No Fraud:{prediction_no_fraud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:902513, batch_size: 512\n",
      "Epoch [1/200], Loss: 0.3709\n",
      "Epoch [2/200], Loss: 0.3635\n",
      "Epoch [3/200], Loss: 0.3611\n",
      "Epoch [4/200], Loss: 0.3583\n",
      "Epoch [5/200], Loss: 0.3557\n",
      "Epoch [6/200], Loss: 0.3541\n",
      "Epoch [7/200], Loss: 0.3537\n",
      "Epoch [8/200], Loss: 0.3535\n",
      "Epoch [9/200], Loss: 0.3532\n",
      "Epoch [10/200], Loss: 0.3538\n",
      "Epoch [11/200], Loss: 0.3533\n",
      "Epoch [12/200], Loss: 0.3533\n",
      "Epoch [13/200], Loss: 0.3532\n",
      "Epoch [14/200], Loss: 0.3527\n",
      "Epoch [15/200], Loss: 0.3523\n",
      "Epoch [16/200], Loss: 0.3523\n",
      "Epoch [17/200], Loss: 0.3518\n",
      "Epoch [18/200], Loss: 0.3513\n",
      "Epoch [19/200], Loss: 0.3508\n",
      "Epoch [20/200], Loss: 0.3501\n",
      "Epoch [21/200], Loss: 0.3496\n",
      "Epoch [22/200], Loss: 0.3489\n",
      "Epoch [23/200], Loss: 0.3489\n",
      "Epoch [24/200], Loss: 0.3490\n",
      "Epoch [25/200], Loss: 0.3487\n",
      "Epoch [26/200], Loss: 0.3485\n",
      "Epoch [27/200], Loss: 0.3488\n",
      "Epoch [28/200], Loss: 0.3485\n",
      "Epoch [29/200], Loss: 0.3483\n",
      "Epoch [30/200], Loss: 0.3482\n",
      "Epoch [31/200], Loss: 0.3482\n",
      "Epoch [32/200], Loss: 0.3491\n",
      "Epoch [33/200], Loss: 0.3484\n",
      "Epoch [34/200], Loss: 0.3487\n",
      "Epoch [35/200], Loss: 0.3492\n",
      "Epoch [36/200], Loss: 0.3488\n",
      "Epoch [37/200], Loss: 0.3499\n",
      "Epoch [38/200], Loss: 0.3499\n",
      "Epoch [39/200], Loss: 0.3500\n",
      "Epoch [40/200], Loss: 0.3499\n",
      "Epoch [41/200], Loss: 0.3496\n",
      "Epoch [42/200], Loss: 0.3488\n",
      "Epoch [43/200], Loss: 0.3503\n",
      "Epoch [44/200], Loss: 0.3498\n",
      "Epoch [45/200], Loss: 0.3490\n",
      "Epoch [46/200], Loss: 0.3497\n",
      "Epoch [47/200], Loss: 0.3489\n",
      "Epoch [48/200], Loss: 0.3494\n",
      "Epoch [49/200], Loss: 0.3496\n",
      "Epoch [50/200], Loss: 0.3496\n",
      "Epoch [51/200], Loss: 0.3495\n",
      "Epoch [52/200], Loss: 0.3506\n",
      "Epoch [53/200], Loss: 0.3509\n",
      "Epoch [54/200], Loss: 0.3502\n",
      "Epoch [55/200], Loss: 0.3498\n",
      "Epoch [56/200], Loss: 0.3499\n",
      "Epoch [57/200], Loss: 0.3500\n",
      "Epoch [58/200], Loss: 0.3494\n",
      "Epoch [59/200], Loss: 0.3499\n",
      "Epoch [60/200], Loss: 0.3497\n",
      "Epoch [61/200], Loss: 0.3496\n",
      "Epoch [62/200], Loss: 0.3505\n",
      "Epoch [63/200], Loss: 0.3506\n",
      "Epoch [64/200], Loss: 0.3493\n",
      "Epoch [65/200], Loss: 0.3492\n",
      "Epoch [66/200], Loss: 0.3490\n",
      "Epoch [67/200], Loss: 0.3496\n",
      "Epoch [68/200], Loss: 0.3502\n",
      "Epoch [69/200], Loss: 0.3495\n",
      "Epoch [70/200], Loss: 0.3499\n",
      "Epoch [71/200], Loss: 0.3491\n",
      "Epoch [72/200], Loss: 0.3491\n",
      "Epoch [73/200], Loss: 0.3486\n",
      "Epoch [74/200], Loss: 0.3490\n",
      "Epoch [75/200], Loss: 0.3491\n",
      "Epoch [76/200], Loss: 0.3487\n",
      "Epoch [77/200], Loss: 0.3481\n",
      "Epoch [78/200], Loss: 0.3486\n",
      "Epoch [79/200], Loss: 0.3490\n",
      "Epoch [80/200], Loss: 0.3478\n",
      "Epoch [81/200], Loss: 0.3480\n",
      "Epoch [82/200], Loss: 0.3479\n",
      "Epoch [83/200], Loss: 0.3488\n",
      "Epoch [84/200], Loss: 0.3481\n",
      "Epoch [85/200], Loss: 0.3482\n",
      "Epoch [86/200], Loss: 0.3480\n",
      "Epoch [87/200], Loss: 0.3480\n",
      "Epoch [88/200], Loss: 0.3481\n",
      "Epoch [89/200], Loss: 0.3482\n",
      "Epoch [90/200], Loss: 0.3483\n",
      "Epoch [91/200], Loss: 0.3481\n",
      "Epoch [92/200], Loss: 0.3481\n",
      "Epoch [93/200], Loss: 0.3482\n",
      "Epoch [94/200], Loss: 0.3482\n",
      "Epoch [95/200], Loss: 0.3484\n",
      "Epoch [96/200], Loss: 0.3490\n",
      "Epoch [97/200], Loss: 0.3482\n",
      "Epoch [98/200], Loss: 0.3480\n",
      "Epoch [99/200], Loss: 0.3474\n",
      "Epoch [100/200], Loss: 0.3467\n",
      "Epoch [101/200], Loss: 0.3467\n",
      "Epoch [102/200], Loss: 0.3479\n",
      "Epoch [103/200], Loss: 0.3473\n",
      "Epoch [104/200], Loss: 0.3469\n",
      "Epoch [105/200], Loss: 0.3468\n",
      "Epoch [106/200], Loss: 0.3463\n",
      "Epoch [107/200], Loss: 0.3462\n",
      "Epoch [108/200], Loss: 0.3467\n",
      "Epoch [109/200], Loss: 0.3464\n",
      "Epoch [110/200], Loss: 0.3467\n",
      "Epoch [111/200], Loss: 0.3468\n",
      "Epoch [112/200], Loss: 0.3467\n",
      "Epoch [113/200], Loss: 0.3465\n",
      "Epoch [114/200], Loss: 0.3470\n",
      "Epoch [115/200], Loss: 0.3468\n",
      "Epoch [116/200], Loss: 0.3473\n",
      "Epoch [117/200], Loss: 0.3464\n",
      "Epoch [118/200], Loss: 0.3473\n",
      "Epoch [119/200], Loss: 0.3463\n",
      "Epoch [120/200], Loss: 0.3467\n",
      "Epoch [121/200], Loss: 0.3467\n",
      "Epoch [122/200], Loss: 0.3468\n",
      "Epoch [123/200], Loss: 0.3468\n",
      "Epoch [124/200], Loss: 0.3468\n",
      "Epoch [125/200], Loss: 0.3472\n",
      "Epoch [126/200], Loss: 0.3468\n",
      "Epoch [127/200], Loss: 0.3469\n",
      "Epoch [128/200], Loss: 0.3474\n",
      "Epoch [129/200], Loss: 0.3472\n",
      "Epoch [130/200], Loss: 0.3468\n",
      "Epoch [131/200], Loss: 0.3466\n",
      "Epoch [132/200], Loss: 0.3475\n",
      "Epoch [133/200], Loss: 0.3466\n",
      "Epoch [134/200], Loss: 0.3452\n",
      "Epoch [135/200], Loss: 0.3455\n",
      "Epoch [136/200], Loss: 0.3458\n",
      "Epoch [137/200], Loss: 0.3458\n",
      "Epoch [138/200], Loss: 0.3459\n",
      "Epoch [139/200], Loss: 0.3455\n",
      "Epoch [140/200], Loss: 0.3451\n",
      "Epoch [141/200], Loss: 0.3458\n",
      "Epoch [142/200], Loss: 0.3451\n",
      "Epoch [143/200], Loss: 0.3458\n",
      "Epoch [144/200], Loss: 0.3441\n",
      "Epoch [145/200], Loss: 0.3445\n",
      "Epoch [146/200], Loss: 0.3448\n",
      "Epoch [147/200], Loss: 0.3450\n",
      "Epoch [148/200], Loss: 0.3456\n",
      "Epoch [149/200], Loss: 0.3446\n",
      "Epoch [150/200], Loss: 0.3462\n",
      "Epoch [151/200], Loss: 0.3458\n",
      "Epoch [152/200], Loss: 0.3454\n",
      "Epoch [153/200], Loss: 0.3456\n",
      "Epoch [154/200], Loss: 0.3457\n",
      "Epoch [155/200], Loss: 0.3459\n",
      "Epoch [156/200], Loss: 0.3466\n",
      "Epoch [157/200], Loss: 0.3445\n",
      "Epoch [158/200], Loss: 0.3461\n",
      "Epoch [159/200], Loss: 0.3454\n",
      "Epoch [160/200], Loss: 0.3461\n",
      "Epoch [161/200], Loss: 0.3459\n",
      "Epoch [162/200], Loss: 0.3466\n",
      "Epoch [163/200], Loss: 0.3463\n",
      "Epoch [164/200], Loss: 0.3463\n",
      "Epoch [165/200], Loss: 0.3455\n",
      "Epoch [166/200], Loss: 0.3464\n",
      "Epoch [167/200], Loss: 0.3452\n",
      "Epoch [168/200], Loss: 0.3466\n",
      "Epoch [169/200], Loss: 0.3473\n",
      "Epoch [170/200], Loss: 0.3466\n",
      "Epoch [171/200], Loss: 0.3458\n",
      "Epoch [172/200], Loss: 0.3470\n",
      "Epoch [173/200], Loss: 0.3465\n",
      "Epoch [174/200], Loss: 0.3475\n",
      "Epoch [175/200], Loss: 0.3470\n",
      "Epoch [176/200], Loss: 0.3473\n",
      "Epoch [177/200], Loss: 0.3461\n",
      "Epoch [178/200], Loss: 0.3460\n",
      "Epoch [179/200], Loss: 0.3476\n",
      "Epoch [180/200], Loss: 0.3457\n",
      "Epoch [181/200], Loss: 0.3473\n",
      "Epoch [182/200], Loss: 0.3468\n",
      "Epoch [183/200], Loss: 0.3469\n",
      "Epoch [184/200], Loss: 0.3478\n",
      "Epoch [185/200], Loss: 0.3474\n",
      "Epoch [186/200], Loss: 0.3472\n",
      "Epoch [187/200], Loss: 0.3480\n",
      "Epoch [188/200], Loss: 0.3468\n",
      "Epoch [189/200], Loss: 0.3457\n",
      "Epoch [190/200], Loss: 0.3464\n",
      "Epoch [191/200], Loss: 0.3470\n",
      "Epoch [192/200], Loss: 0.3463\n",
      "Epoch [193/200], Loss: 0.3462\n",
      "Epoch [194/200], Loss: 0.3471\n",
      "Epoch [195/200], Loss: 0.3459\n",
      "Epoch [196/200], Loss: 0.3458\n",
      "Epoch [197/200], Loss: 0.3468\n",
      "Epoch [198/200], Loss: 0.3476\n",
      "Epoch [199/200], Loss: 0.3467\n",
      "Epoch [200/200], Loss: 0.3463\n",
      "Accuracy: 0.9745942014612053\n"
     ]
    }
   ],
   "source": [
    "##------------------------------------------\n",
    "##             PyTorch \n",
    "##------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = transactions_df[transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")]\n",
    "\n",
    "# Separate features and target\n",
    "columns = list(transactions_df.columns)\n",
    "\n",
    "# Entries to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                     'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                     'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                    ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "\n",
    "target = 'fraud'\n",
    "y = data[target]\n",
    "\n",
    "features = [col for col in columns if col not in ['post_ts']]\n",
    "\n",
    "\n",
    "# Save feature names and output format to a JSON file\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'target': target,\n",
    "    'model_type':  'PyTorch',\n",
    "    'model_name': 'PyTorch Model Example',\n",
    "    'model_version': 1.0,\n",
    "    'scaler_file': './models/pytorch-ffn-model/pytorch-ffn-scaler.pkl'\n",
    "}\n",
    "\n",
    "with open('./models/pytorch-ffn-model/pytorch-ffn-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "\n",
    "# Define the training range max limit\n",
    "end_training = pd.Timestamp('2023-05-31')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = X[X['post_ts'] <= end_training]\n",
    "y_train = y.loc[X_train.index].tolist()\n",
    "\n",
    "X_test = X[X['post_ts'] > end_training]\n",
    "y_test = y.loc[X_test.index].tolist()\n",
    "\n",
    "# Drop the 'post_ts' column\n",
    "X_train = X_train.drop(columns=['post_ts'])\n",
    "X_test = X_test.drop(columns=['post_ts'])\n",
    "\n",
    "\n",
    "# Standardize features (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler to a file\n",
    "joblib.dump(scaler, metadata['scaler_file'])\n",
    "\n",
    "# Convert back to PyTorch tensors after scaling\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# # Create a TensorDataset and DataLoader\n",
    "# dataset = TensorDataset(X_train, y_tensor)\n",
    "# train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "ffn = FeedforwardNN(X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(ffn.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 200\n",
    "batch_size =  512 #math.ceil(len(X_train)/epochs) #512\n",
    "\n",
    "print(f\"size:{len(X_train)}, batch_size: {batch_size}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ffn(inputs)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = ffn(X_test).numpy()\n",
    "    predictions = (predictions > 0.5).astype(float)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test.numpy(), predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "ffn.eval()\n",
    "# Export to TorchScript\n",
    "model_scripted = torch.jit.script(ffn) \n",
    "# Save the trained model\n",
    "model_scripted.save('./models/pytorch-ffn-model/pytorch-ffn-model.pt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud:0.7296394109725952, No Fraud: 0.04654536023736\n"
     ]
    }
   ],
   "source": [
    "model = torch.jit.load('./models/pytorch-ffn-model/pytorch-ffn-model.pt')\n",
    "\n",
    "# Read the metadata file\n",
    "with open('./models/pytorch-ffn-model/pytorch-ffn-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "scaler_file = metadata['scaler_file']\n",
    "\n",
    "scaler = joblib.load(scaler_file)\n",
    "\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    {'amt':[141.45], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[3], \n",
    "            'customer_id_avg_amount_1day_window':[88.08],\n",
    "            'customer_id_nb_tx_7day_window':[20.0], \n",
    "            'customer_id_avg_amount_7day_window':[64.4855],\n",
    "            'customer_id_nb_tx_30day_window':[82.0], \n",
    "            'customer_id_avg_amount_30day_window':[59.64829268292683],\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            })\n",
    "\n",
    "new_record_fraud = scaler.transform(new_record_fraud)\n",
    "new_record_no_fraud = scaler.transform(new_record_no_fraud)\n",
    "\n",
    "# Convert back to PyTorch tensors after scaling\n",
    "new_record_fraud = torch.tensor(new_record_fraud, dtype=torch.float32)\n",
    "new_record_no_fraud = torch.tensor(new_record_no_fraud, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the prediction for the new record\n",
    "# Set the model to evaluation mode (important for models with dropout or batch normalization) \n",
    "model.eval()\n",
    "# Disable gradient computation.\n",
    "with torch.no_grad():\n",
    "    predictions_fraud = model(new_record_fraud)[0][0].item()\n",
    "    # predictions_fraud = (predictions_fraud > 0.5) #.astype(float)\n",
    "\n",
    "    predictions_no_fraud = model(new_record_no_fraud)[0][0].item()\n",
    "    # predictions_no_fraud = (predictions_no_fraud > 0.5) #.astype(float)\n",
    "\n",
    "\n",
    "print(f\"Fraud:{predictions_fraud}, No Fraud: {predictions_no_fraud}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 9\n",
      "batch_size: 512\n",
      "Epoch [1/50], Loss: 0.2165\n",
      "Epoch [2/50], Loss: 0.1258\n",
      "Epoch [3/50], Loss: 0.1238\n",
      "Epoch [4/50], Loss: 0.1267\n",
      "Epoch [5/50], Loss: 0.1225\n",
      "Epoch [6/50], Loss: 0.1200\n",
      "Epoch [7/50], Loss: 0.1186\n",
      "Epoch [8/50], Loss: 0.1172\n",
      "Epoch [9/50], Loss: 0.1170\n",
      "Epoch [10/50], Loss: 0.1167\n",
      "Epoch [11/50], Loss: 0.1163\n",
      "Epoch [12/50], Loss: 0.1161\n",
      "Epoch [13/50], Loss: 0.1159\n",
      "Epoch [14/50], Loss: 0.1159\n",
      "Epoch [15/50], Loss: 0.1157\n",
      "Epoch [16/50], Loss: 0.1157\n",
      "Epoch [17/50], Loss: 0.1156\n",
      "Epoch [18/50], Loss: 0.1156\n",
      "Epoch [19/50], Loss: 0.1156\n",
      "Epoch [20/50], Loss: 0.1156\n",
      "Epoch [21/50], Loss: 0.1156\n",
      "Epoch [22/50], Loss: 0.1156\n",
      "Epoch [23/50], Loss: 0.1155\n",
      "Epoch [24/50], Loss: 0.1155\n",
      "Epoch [25/50], Loss: 0.1154\n",
      "Epoch [26/50], Loss: 0.1154\n",
      "Epoch [27/50], Loss: 0.1155\n",
      "Epoch [28/50], Loss: 0.1158\n",
      "Epoch [29/50], Loss: 0.1177\n",
      "Epoch [30/50], Loss: 0.1207\n",
      "Epoch [31/50], Loss: 0.1257\n",
      "Epoch [32/50], Loss: 0.1273\n",
      "Epoch [33/50], Loss: 0.1284\n",
      "Epoch [34/50], Loss: 0.1277\n",
      "Epoch [35/50], Loss: 0.1272\n",
      "Epoch [36/50], Loss: 0.1269\n",
      "Epoch [37/50], Loss: 0.1263\n",
      "Epoch [38/50], Loss: 0.1258\n",
      "Epoch [39/50], Loss: 0.1254\n",
      "Epoch [40/50], Loss: 0.1249\n",
      "Epoch [41/50], Loss: 0.1242\n",
      "Epoch [42/50], Loss: 0.1238\n",
      "Epoch [43/50], Loss: 0.1235\n",
      "Epoch [44/50], Loss: 0.1233\n",
      "Epoch [45/50], Loss: 0.1231\n",
      "Epoch [46/50], Loss: 0.1230\n",
      "Epoch [47/50], Loss: 0.1229\n",
      "Epoch [48/50], Loss: 0.1227\n",
      "Epoch [49/50], Loss: 0.1226\n",
      "Epoch [50/50], Loss: 0.1225\n",
      "0.31190311908721924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/autoencoder-model/autoencoder-scaler.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset Only the no fraud records\n",
    "data = transactions_df[(transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")) & \n",
    "                    #    (transactions_df['post_ts'] < pd.Timestamp(\"2023-05-01\")) & \n",
    "                       (data['fraud'] == 0) ]\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "columns = list(transactions_df.columns)\n",
    "\n",
    "# Entries to remove\n",
    "entries_to_remove = ['transaction_id', 'post_ts', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                     'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                     'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                    ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "print(f\"Size: {X_tensor.shape[1]}\")\n",
    "\n",
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(4, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(2, 4),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(4, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, input_size),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "autenc = Autoencoder(X_tensor.shape[1])\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(autenc.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "batch_size =  512 # math.ceil(len(X_tensor)*3/epochs) #512 len(X_tensor) # \n",
    "print(f\"batch_size: {batch_size}\")\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        inputs = X_tensor[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autenc(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "with torch.no_grad():\n",
    "    reconstructed = autenc(X_tensor)\n",
    "\n",
    "    # print(f\"X_tensor: {X_tensor}\")\n",
    "    # print(f\"reconstructed: {reconstructed}\")\n",
    "    \n",
    "    # reconstruction_error = torch.nn.functional.mse_loss(reconstructed, X_tensor, reduction='mean')\n",
    "    reconstruction_error = torch.mean(torch.abs(X_tensor - reconstructed), dim=1).numpy()\n",
    "\n",
    "# threshold = reconstruction_error.item()\n",
    "# print(np.mean(reconstruction_error), np.std(reconstruction_error))\n",
    "threshold = np.mean(reconstruction_error) + 1 * np.std(reconstruction_error)  # 1 standard deviation above the mean\n",
    "print(threshold)\n",
    "\n",
    "# Save model\n",
    "# torch.save(model.state_dict(), './autoencoder-model/autoencoder-model.pt')\n",
    "autenc.eval()\n",
    "# Export to TorchScript\n",
    "model_scripted = torch.jit.script(autenc) \n",
    "# Save the trained model\n",
    "model_scripted.save('./models/autoencoder-model/autoencoder-model.pt') \n",
    "\n",
    "# Optional: Save metadata and scaler\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'model_type': 'PyTorch Autoencoder',\n",
    "    'model_name': 'Autoencoder for Anomaly Detection',\n",
    "    'model_version': 1.0,\n",
    "    'threshold': float(threshold),\n",
    "    'scaler_file': './models/autoencoder-model/autoencoder-scaler.pkl'\n",
    "}\n",
    "with open('./models/autoencoder-model/autoencoder-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "joblib.dump(scaler, metadata['scaler_file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud:5.178807735443115, No Fraud: 0.22883540391921997, threshold: 0.31190311908721924\n"
     ]
    }
   ],
   "source": [
    "model = torch.jit.load('./models/autoencoder-model/autoencoder-model.pt')\n",
    "\n",
    "# Read the metadata file\n",
    "with open('./models/autoencoder-model/autoencoder-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "scaler_file = metadata['scaler_file']\n",
    "threshold = metadata['threshold']\n",
    "\n",
    "scaler = joblib.load(scaler_file)\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    {'amt':[ 99.4], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[4], \n",
    "            'customer_id_avg_amount_1day_window':[430.22],\n",
    "            'customer_id_nb_tx_7day_window':[6], \n",
    "            'customer_id_avg_amount_7day_window':[602.15],\n",
    "            'customer_id_nb_tx_30day_window':[6], \n",
    "            'customer_id_avg_amount_30day_window':[602.15],\n",
    "            })\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            })\n",
    "\n",
    "new_record_fraud = scaler.transform(new_record_fraud)\n",
    "new_record_no_fraud = scaler.transform(new_record_no_fraud)\n",
    "\n",
    "# Convert back to PyTorch tensors after scaling\n",
    "new_record_fraud = torch.tensor(new_record_fraud, dtype=torch.float32)\n",
    "new_record_no_fraud = torch.tensor(new_record_no_fraud, dtype=torch.float32)\n",
    "\n",
    "# Get the prediction for the new record\n",
    "# Set the model to evaluation mode (important for models with dropout or batch normalization) \n",
    "model.eval()\n",
    "# Disable gradient computation.\n",
    "with torch.no_grad():\n",
    "    reconstructed_fraud = model(new_record_fraud)\n",
    "    reconstruction_fraud_error = np.mean(torch.mean(torch.abs(reconstructed_fraud - new_record_fraud), dim=1).numpy())\n",
    "    # reconstruction_fraud_error = torch.nn.functional.mse_loss(reconstructed_fraud, new_record_fraud, reduction='mean')\n",
    "\n",
    "    reconstructed_no_fraud = model(new_record_no_fraud)\n",
    "    reconstruction_no_fraud_error = np.mean(torch.mean(torch.abs(reconstructed_no_fraud - new_record_no_fraud), dim=1).numpy())\n",
    "    # reconstruction_no_fraud_error = torch.nn.functional.mse_loss(reconstructed_no_fraud, new_record_no_fraud, reduction='mean')\n",
    "\n",
    "\n",
    "print(f\"Fraud:{reconstruction_fraud_error}, No Fraud: {reconstruction_no_fraud_error}, threshold: {threshold}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
