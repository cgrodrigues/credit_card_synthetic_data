{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m transactions_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/transactions_enrich_df.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost_ts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/DataKensei/tornado_backend/ENV/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/DataKensei/tornado_backend/ENV/lib/python3.8/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/DataKensei/tornado_backend/ENV/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/projects/DataKensei/tornado_backend/ENV/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:272\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    270\u001b[0m names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleading_cols\u001b[49m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_complex_date_col:\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile structure not yet supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "\n",
    "transactions_df = pd.read_csv(\"./data/transactions_enrich_df.csv\", parse_dates = ['post_ts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn - RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------\n",
    "##             Scikit-Learn   \n",
    "##------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "data = transactions_df[transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")]\n",
    "\n",
    "# Separate features (F1, F2, F3) and target (TARGET)\n",
    "columns =  list(transactions_df.columns)\n",
    "\n",
    "# Entries you want to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode', \n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                        'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                        'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                        ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "target = 'fraud'\n",
    "y = data[target]\n",
    "\n",
    "# Define the traing range max limit\n",
    "end_training = pd.Timestamp('2023-05-31')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = X[X['post_ts'] <= end_training]\n",
    "y_train = y.loc[X_train.index]\n",
    "X_test = X[X['post_ts'] > end_training]\n",
    "y_test =  y.loc[X_test.index]\n",
    "\n",
    "# Drop teh column post_ts\n",
    "X_train = X_train.drop(columns=['post_ts'])\n",
    "X_test = X_test.drop(columns=['post_ts'])\n",
    "\n",
    "# Save feature names and target name to a JSON file\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'target': target,\n",
    "    'model_name': 'RandomForestClassifier Model Example',\n",
    "    'model_version': 1.0,\n",
    "    'model_type':  'Scikit-Learn'\n",
    "}\n",
    "\n",
    "with open('./scikit-learn-random-forest-model/scikit-learn-random-forest-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, './scikit-learn-random-forest-model/scikit-learn-random-forest-model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = joblib.load('./scikit-learn-random-forest-model/scikit-learn-random-forest-model.pkl')\n",
    "# Read the metadata file\n",
    "with open('./scikit-learn-random-forest-model/scikit-learn-random-forest-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    {'amt':[141.45], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[3], \n",
    "            'customer_id_avg_amount_1day_window':[88.08],\n",
    "            'customer_id_nb_tx_7day_window':[20.0], \n",
    "            'customer_id_avg_amount_7day_window':[64.4855],\n",
    "            'customer_id_nb_tx_30day_window':[82.0], \n",
    "            'customer_id_avg_amount_30day_window':[59.64829268292683],\n",
    "            # 'terminal_id_nb_tx_1day_window':[43.0], \n",
    "            # 'terminal_id_risk_1day_window':[0.09302325581395349],\n",
    "            # 'terminal_id_nb_tx_7day_window':[233.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.04721030042918455],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1013.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.03060217176702863]\n",
    "            })\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            # 'terminal_id_nb_tx_1day_window':[31.0], \n",
    "            # 'terminal_id_risk_1day_window':[0],\n",
    "            # 'terminal_id_nb_tx_7day_window':[252.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.03571428571428571],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1166.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.0274442538593482]\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "# Get the cluster assignment for the new record\n",
    "prediction_fraud = model.predict(new_record_fraud)[0]\n",
    "prediction_no_fraud = model.predict(new_record_no_fraud)[0]\n",
    "\n",
    "\n",
    "print (f\"Fraud:{prediction_fraud} - No Fraud:{prediction_no_fraud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "model = joblib.load('./scikit-learn-random-forest-model/scikit-learn-random-forest-model.pkl')\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Separate features (F1, F2, F3) and target (TARGET)\n",
    "columns =  list(transactions_df.columns)\n",
    "\n",
    "# Entries you want to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                        'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                        'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window',\n",
    "                        'post_ts'\n",
    "                        ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "# Convert the importances into a DataFrame\n",
    "feature_importance_df = pd.DataFrame({'feature': features, 'importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "feature_importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = transactions_df[transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")]\n",
    "\n",
    "# Separate features and target\n",
    "columns = list(transactions_df.columns)\n",
    "\n",
    "# Entries to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                     'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                     'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                    ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "target = 'fraud'\n",
    "y = data[target]\n",
    "\n",
    "# Define the training range max limit\n",
    "end_training = pd.Timestamp('2023-05-31')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = X[X['post_ts'] <= end_training]\n",
    "y_train = y.loc[X_train.index]\n",
    "X_test = X[X['post_ts'] > end_training]\n",
    "y_test = y.loc[X_test.index]\n",
    "\n",
    "# Drop the 'post_ts' column\n",
    "X_train = X_train.drop(columns=['post_ts'])\n",
    "X_test = X_test.drop(columns=['post_ts'])\n",
    "\n",
    "# Save feature names and target name to a JSON file\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'target': target,\n",
    "    'model_name': 'XGBoost Model Example',\n",
    "    'model_version': 1.0,\n",
    "    'model_type': 'XGBoost'\n",
    "}\n",
    "\n",
    "with open('./xgboost-model/xgboost-model-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "clf = XGBClassifier(n_estimators=10, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, './xgboost-model/xgboost-model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = joblib.load('./xgboost-model/xgboost-model.pkl')\n",
    "# Read the metadata file\n",
    "with open('./xgboost-model/xgboost-model-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    {'amt':[141.45], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[3], \n",
    "            'customer_id_avg_amount_1day_window':[88.08],\n",
    "            'customer_id_nb_tx_7day_window':[20.0], \n",
    "            'customer_id_avg_amount_7day_window':[64.4855],\n",
    "            'customer_id_nb_tx_30day_window':[82.0], \n",
    "            'customer_id_avg_amount_30day_window':[59.64829268292683],\n",
    "            # 'terminal_id_nb_tx_1day_window':[43.0], \n",
    "            # 'terminal_id_risk_1day_window':[0.09302325581395349],\n",
    "            # 'terminal_id_nb_tx_7day_window':[233.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.04721030042918455],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1013.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.03060217176702863]\n",
    "            })\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            # 'terminal_id_nb_tx_1day_window':[31.0], \n",
    "            # 'terminal_id_risk_1day_window':[0],\n",
    "            # 'terminal_id_nb_tx_7day_window':[252.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.03571428571428571],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1166.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.0274442538593482]\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "# Get the cluster assignment for the new record\n",
    "prediction_fraud = model.predict(new_record_fraud)[0]\n",
    "prediction_no_fraud = model.predict(new_record_no_fraud)[0]\n",
    "\n",
    "\n",
    "print (f\"Fraud:{prediction_fraud} - No Fraud:{prediction_no_fraud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------\n",
    "##             PyTorch \n",
    "##------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = transactions_df[transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")]\n",
    "\n",
    "# Separate features and target\n",
    "columns = list(transactions_df.columns)\n",
    "\n",
    "# Entries to remove\n",
    "entries_to_remove = ['transaction_id', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                     'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                     'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                    ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "\n",
    "target = 'fraud'\n",
    "y = data[target]\n",
    "\n",
    "features = [col for col in columns if col not in ['post_ts']]\n",
    "\n",
    "\n",
    "# Save feature names and output format to a JSON file\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'target': target,\n",
    "    'model_type':  'PyTorch',\n",
    "    'model_name': 'PyTorch Model Example',\n",
    "    'model_version': 1.0,\n",
    "    'scaler_file': './pytorch-ffn-model/pytorch-ffn-scaler.pkl'\n",
    "}\n",
    "\n",
    "with open('./pytorch-ffn-model/pytorch-ffn-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "\n",
    "# Define the training range max limit\n",
    "end_training = pd.Timestamp('2023-05-31')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = X[X['post_ts'] <= end_training]\n",
    "y_train = y.loc[X_train.index].tolist()\n",
    "\n",
    "X_test = X[X['post_ts'] > end_training]\n",
    "y_test = y.loc[X_test.index].tolist()\n",
    "\n",
    "# Drop the 'post_ts' column\n",
    "X_train = X_train.drop(columns=['post_ts'])\n",
    "X_test = X_test.drop(columns=['post_ts'])\n",
    "\n",
    "\n",
    "# Standardize features (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler to a file\n",
    "joblib.dump(scaler, metadata['scaler_file'])\n",
    "\n",
    "# Convert back to PyTorch tensors after scaling\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# # Create a TensorDataset and DataLoader\n",
    "# dataset = TensorDataset(X_train, y_tensor)\n",
    "# train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "ffn = FeedforwardNN(X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(ffn.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 200\n",
    "batch_size =  512 #math.ceil(len(X_train)/epochs) #512\n",
    "\n",
    "print(f\"size:{len(X_train)}, batch_size: {batch_size}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ffn(inputs)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = ffn(X_test).numpy()\n",
    "    predictions = (predictions > 0.5).astype(float)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test.numpy(), predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "ffn.eval()\n",
    "# Export to TorchScript\n",
    "model_scripted = torch.jit.script(ffn) \n",
    "# Save the trained model\n",
    "model_scripted.save('./pytorch-ffn-model/pytorch-ffn-model.pt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('./pytorch-ffn-model/pytorch-ffn-model.pt')\n",
    "\n",
    "# Read the metadata file\n",
    "with open('./pytorch-ffn-model/pytorch-ffn-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "scaler_file = metadata['scaler_file']\n",
    "\n",
    "scaler = joblib.load(scaler_file)\n",
    "\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    {'amt':[141.45], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[3], \n",
    "            'customer_id_avg_amount_1day_window':[88.08],\n",
    "            'customer_id_nb_tx_7day_window':[20.0], \n",
    "            'customer_id_avg_amount_7day_window':[64.4855],\n",
    "            'customer_id_nb_tx_30day_window':[82.0], \n",
    "            'customer_id_avg_amount_30day_window':[59.64829268292683],\n",
    "            # 'terminal_id_nb_tx_1day_window':[43.0], \n",
    "            # 'terminal_id_risk_1day_window':[0.09302325581395349],\n",
    "            # 'terminal_id_nb_tx_7day_window':[233.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.04721030042918455],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1013.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.03060217176702863]\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            # 'terminal_id_nb_tx_1day_window':[31.0], \n",
    "            # 'terminal_id_risk_1day_window':[0],\n",
    "            # 'terminal_id_nb_tx_7day_window':[252.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.03571428571428571],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1166.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.0274442538593482]\n",
    "            })\n",
    "\n",
    "new_record_fraud = scaler.transform(new_record_fraud)\n",
    "new_record_no_fraud = scaler.transform(new_record_no_fraud)\n",
    "\n",
    "# Convert back to PyTorch tensors after scaling\n",
    "new_record_fraud = torch.tensor(new_record_fraud, dtype=torch.float32)\n",
    "new_record_no_fraud = torch.tensor(new_record_no_fraud, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the prediction for the new record\n",
    "# Set the model to evaluation mode (important for models with dropout or batch normalization) \n",
    "model.eval()\n",
    "# Disable gradient computation.\n",
    "with torch.no_grad():\n",
    "    predictions_fraud = model(new_record_fraud)[0][0].item()\n",
    "    # predictions_fraud = (predictions_fraud > 0.5) #.astype(float)\n",
    "\n",
    "    predictions_no_fraud = model(new_record_no_fraud)[0][0].item()\n",
    "    # predictions_no_fraud = (predictions_no_fraud > 0.5) #.astype(float)\n",
    "\n",
    "\n",
    "print(f\"Fraud:{predictions_fraud}, No Fraud: {predictions_no_fraud}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset Only the no fraud records\n",
    "data = transactions_df[(transactions_df['post_ts'] > pd.Timestamp(\"2023-03-01\")) & \n",
    "                    #    (transactions_df['post_ts'] < pd.Timestamp(\"2023-05-01\")) & \n",
    "                       (data['fraud'] == 0) ]\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "columns = list(transactions_df.columns)\n",
    "\n",
    "# Entries to remove\n",
    "entries_to_remove = ['transaction_id', 'post_ts', \n",
    "                     'customer_id', 'bin', 'entry_mode',\n",
    "                     'terminal_id', 'fraud',\n",
    "                     'fraud_scenario',\n",
    "                     'terminal_id_nb_tx_1day_window', 'terminal_id_risk_1day_window',\n",
    "                     'terminal_id_nb_tx_7day_window', 'terminal_id_risk_7day_window',\n",
    "                     'terminal_id_nb_tx_30day_window', 'terminal_id_risk_30day_window'\n",
    "                    ]  \n",
    "\n",
    "# Remove the entries\n",
    "features = [col for col in columns if col not in entries_to_remove]\n",
    "\n",
    "X = data[features]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "print(f\"Size: {X_tensor.shape[1]}\")\n",
    "\n",
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(4, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(2, 4),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(4, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, input_size),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "autenc = Autoencoder(X_tensor.shape[1])\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(autenc.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "batch_size =  512 # math.ceil(len(X_tensor)*3/epochs) #512 len(X_tensor) # \n",
    "print(f\"batch_size: {batch_size}\")\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        inputs = X_tensor[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autenc(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "with torch.no_grad():\n",
    "    reconstructed = autenc(X_tensor)\n",
    "\n",
    "    # print(f\"X_tensor: {X_tensor}\")\n",
    "    # print(f\"reconstructed: {reconstructed}\")\n",
    "    \n",
    "    # reconstruction_error = torch.nn.functional.mse_loss(reconstructed, X_tensor, reduction='mean')\n",
    "    reconstruction_error = torch.mean(torch.abs(X_tensor - reconstructed), dim=1).numpy()\n",
    "\n",
    "# threshold = reconstruction_error.item()\n",
    "# print(np.mean(reconstruction_error), np.std(reconstruction_error))\n",
    "threshold = np.mean(reconstruction_error) + 1 * np.std(reconstruction_error)  # 1 standard deviation above the mean\n",
    "print(threshold)\n",
    "\n",
    "# Save model\n",
    "# torch.save(model.state_dict(), './autoencoder-model/autoencoder-model.pt')\n",
    "autenc.eval()\n",
    "# Export to TorchScript\n",
    "model_scripted = torch.jit.script(autenc) \n",
    "# Save the trained model\n",
    "model_scripted.save('./autoencoder-model/autoencoder-model.pt') \n",
    "\n",
    "# Optional: Save metadata and scaler\n",
    "metadata = {\n",
    "    'features': features,\n",
    "    'model_type': 'PyTorch Autoencoder',\n",
    "    'model_name': 'Autoencoder for Anomaly Detection',\n",
    "    'model_version': 1.0,\n",
    "    'threshold': float(threshold),\n",
    "    'scaler_file': './autoencoder-model/autoencoder-scaler.pkl'\n",
    "}\n",
    "with open('./autoencoder-model/autoencoder-metadata.json', 'w') as metadata_file:\n",
    "    json.dump(metadata, metadata_file)\n",
    "\n",
    "joblib.dump(scaler, metadata['scaler_file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('./autoencoder-model/autoencoder-model.pt')\n",
    "\n",
    "# Read the metadata file\n",
    "with open('./autoencoder-model/autoencoder-metadata.json', 'r') as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "model_name = metadata['model_name']\n",
    "model_version = metadata['model_version']\n",
    "scaler_file = metadata['scaler_file']\n",
    "threshold = metadata['threshold']\n",
    "\n",
    "scaler = joblib.load(scaler_file)\n",
    "\n",
    "# Define the new record -- Fraud\n",
    "new_record_fraud = pd.DataFrame(\n",
    "    # {'amt':[141.45], \n",
    "    #         'during_weekend':[0], \n",
    "    #         'during_night':[0],\n",
    "    #         'customer_id_nb_tx_1day_window':[3], \n",
    "    #         'customer_id_avg_amount_1day_window':[88.08],\n",
    "    #         'customer_id_nb_tx_7day_window':[20.0], \n",
    "    #         'customer_id_avg_amount_7day_window':[64.4855],\n",
    "    #         'customer_id_nb_tx_30day_window':[82.0], \n",
    "    #         'customer_id_avg_amount_30day_window':[59.64829268292683],\n",
    "    #         # 'terminal_id_nb_tx_1day_window':[43.0], \n",
    "    #         # 'terminal_id_risk_1day_window':[0.09302325581395349],\n",
    "    #         # 'terminal_id_nb_tx_7day_window':[233.0], \n",
    "    #         # 'terminal_id_risk_7day_window':[0.04721030042918455],\n",
    "    #         # 'terminal_id_nb_tx_30day_window':[1013.0], \n",
    "    #         # 'terminal_id_risk_30day_window':[0.03060217176702863]\n",
    "    #         })\n",
    "    {'amt':[ 99.4], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[4], \n",
    "            'customer_id_avg_amount_1day_window':[430.22],\n",
    "            'customer_id_nb_tx_7day_window':[6], \n",
    "            'customer_id_avg_amount_7day_window':[602.15],\n",
    "            'customer_id_nb_tx_30day_window':[6], \n",
    "            'customer_id_avg_amount_30day_window':[602.15],\n",
    "            # 'terminal_id_nb_tx_1day_window':[43.0], \n",
    "            # 'terminal_id_risk_1day_window':[0.09302325581395349],\n",
    "            # 'terminal_id_nb_tx_7day_window':[233.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.04721030042918455],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1013.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.03060217176702863]\n",
    "            })\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# Define the new record -- No Fraud\n",
    "new_record_no_fraud = pd.DataFrame(\n",
    "    {'amt':[72.33], \n",
    "            'during_weekend':[0], \n",
    "            'during_night':[0],\n",
    "            'customer_id_nb_tx_1day_window':[5], \n",
    "            'customer_id_avg_amount_1day_window':[49.09],\n",
    "            'customer_id_nb_tx_7day_window':[21.0], \n",
    "            'customer_id_avg_amount_7day_window':[49.249],\n",
    "            'customer_id_nb_tx_30day_window':[62.0], \n",
    "            'customer_id_avg_amount_30day_window':[50.29],\n",
    "            # 'terminal_id_nb_tx_1day_window':[31.0], \n",
    "            # 'terminal_id_risk_1day_window':[0],\n",
    "            # 'terminal_id_nb_tx_7day_window':[252.0], \n",
    "            # 'terminal_id_risk_7day_window':[0.03571428571428571],\n",
    "            # 'terminal_id_nb_tx_30day_window':[1166.0], \n",
    "            # 'terminal_id_risk_30day_window':[0.0274442538593482]\n",
    "            })\n",
    "\n",
    "new_record_fraud = scaler.transform(new_record_fraud)\n",
    "new_record_no_fraud = scaler.transform(new_record_no_fraud)\n",
    "\n",
    "# Convert back to PyTorch tensors after scaling\n",
    "new_record_fraud = torch.tensor(new_record_fraud, dtype=torch.float32)\n",
    "new_record_no_fraud = torch.tensor(new_record_no_fraud, dtype=torch.float32)\n",
    "\n",
    "# Get the prediction for the new record\n",
    "# Set the model to evaluation mode (important for models with dropout or batch normalization) \n",
    "model.eval()\n",
    "# Disable gradient computation.\n",
    "with torch.no_grad():\n",
    "    reconstructed_fraud = model(new_record_fraud)\n",
    "    reconstruction_fraud_error = np.mean(torch.mean(torch.abs(reconstructed_fraud - new_record_fraud), dim=1).numpy())\n",
    "    # reconstruction_fraud_error = torch.nn.functional.mse_loss(reconstructed_fraud, new_record_fraud, reduction='mean')\n",
    "\n",
    "    reconstructed_no_fraud = model(new_record_no_fraud)\n",
    "    reconstruction_no_fraud_error = np.mean(torch.mean(torch.abs(reconstructed_no_fraud - new_record_no_fraud), dim=1).numpy())\n",
    "    # reconstruction_no_fraud_error = torch.nn.functional.mse_loss(reconstructed_no_fraud, new_record_no_fraud, reduction='mean')\n",
    "\n",
    "\n",
    "print(f\"Fraud:{reconstruction_fraud_error}, No Fraud: {reconstruction_no_fraud_error}, threshold: {threshold}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
